{"version":3,"file":"llama-chat.js","sourceRoot":"","sources":["llama-chat.ts"],"names":[],"mappings":"AAAA,OAAO,EAAE,QAAQ,EAAE,eAAe,EAAE,iBAAiB,EAAE,MAAM,YAAY,CAAC;AAC1E,OAAO,EAAE,MAAM,EAAE,YAAY,EAAE,MAAM,oBAAoB,CAAC;AAC1D,OAAO,EAAE,YAAY,EAAE,MAAM,IAAI,CAAC;AAClC,OAAO,EAAE,IAAI,EAAE,MAAM,MAAM,CAAC;AAC5B,OAAO,EAAE,MAAM,EAAE,MAAM,QAAQ,CAAC;AAEhC,MAAM,EAAE,CAAC;AAET,MAAM,CAAC,KAAK,UAAU,SAAS,CAC7B,OAAe,EACf,KAAc,EACd,mBAA8D;IAE9D,IAAI,CAAC;QACH,MAAM,MAAM,GAAG,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC;QAC1C,IAAI,CAAC,MAAM;YAAE,MAAM,IAAI,KAAK,CAAC,yBAAyB,CAAC,CAAC;QAExD,6CAA6C;QAC7C,MAAM,YAAY,GAAG,YAAY,CAAC,IAAI,CAAC,OAAO,CAAC,GAAG,EAAE,EAAE,MAAM,EAAE,gBAAgB,CAAC,EAAE,OAAO,CAAC,CAAC;QAE1F,6BAA6B;QAC7B,QAAQ,CAAC,GAAG,GAAG,IAAI,MAAM,CAAC;YACxB,MAAM;YACN,KAAK,EAAE,YAAY,CAAC,wBAAwB;YAC5C,WAAW,EAAE,GAAG;SACjB,CAAC,CAAC;QAEH,+BAA+B;QAC/B,MAAM,KAAK,GAAG,IAAI,eAAe,CAAC;YAChC,IAAI,EAAE,OAAO;YACb,WAAW,EAAE,SAAS;YACtB,cAAc,EAAE,sCAAsC;YACtD,MAAM,EAAE,OAAO,CAAC,GAAG,CAAC,mBAAoB;SACzC,CAAC,CAAC;QAEH,MAAM,SAAS,GAAG,KAAK,CAAC,WAAW,CAAC,EAAE,cAAc,EAAE,CAAC,EAAE,CAAC,CAAC;QAE3D,kDAAkD;QAClD,MAAM,UAAU,GAAG,IAAI,iBAAiB,CAAC;YACvC,SAAS;YACT,YAAY,EAAE,YAAY;SAC3B,CAAC,CAAC;QAEH,wCAAwC;QACxC,IAAI,WAAW,GAAG,OAAO,CAAC;QAE1B,mEAAmE;QACnE,IAAI,mBAAmB,IAAI,mBAAmB,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;YAC1D,OAAO,CAAC,GAAG,CAAC,2CAA2C,mBAAmB,CAAC,MAAM,WAAW,CAAC,CAAC;YAE9F,kDAAkD;YAClD,MAAM,YAAY,GAAG,CAAC,CAAC,CAAC,oDAAoD;YAC5E,MAAM,qBAAqB,GAAG,GAAG,CAAC,CAAC,qCAAqC;YAExE,qCAAqC;YACrC,MAAM,cAAc,GAAG,mBAAmB,CAAC,KAAK,CAAC,CAAC,YAAY,CAAC,CAAC;YAEhE,MAAM,iBAAiB,GAAG,cAAc;iBACrC,MAAM,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,OAAO,GAAG,CAAC,IAAI,KAAK,QAAQ,IAAI,OAAO,GAAG,CAAC,OAAO,KAAK,QAAQ,CAAC;iBACrF,GAAG,CAAC,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;gBAClB,MAAM,gBAAgB,GAAG,GAAG,CAAC,OAAO,CAAC,MAAM,GAAG,qBAAqB;oBACjE,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC,SAAS,CAAC,CAAC,EAAE,qBAAqB,CAAC,GAAG,KAAK,CAAK,CAAC,CAAC,GAAG,CAAC,OAAO,CAAC;gBAC9E,OAAO,GAAG,GAAG,CAAC,IAAI,KAAK,gBAAgB,EAAE,CAAC;YAC5C,CAAC,CAAC,CAAC;YAEL,MAAM,cAAc,GAAG,iBAAiB,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAEpD,OAAO,CAAC,GAAG,CAAC,uBAAuB,iBAAiB,CAAC,MAAM,WAAW,CAAC,CAAC;YAExE,qEAAqE;YACrE,MAAM,kBAAkB,GAAG,OAAO,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,KAAK,CAAC;gBACtC,OAAO,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,SAAS,CAAC;gBACzC,OAAO,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,MAAM,CAAC;gBACtC,OAAO,CAAC,WAAW,EAAE,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC;YAEnE,IAAI,kBAAkB,EAAE,CAAC;gBACvB,WAAW,GAAG,kTAAkT,cAAc,mCAAmC,OAAO,EAAE,CAAC;YAC7X,CAAC;iBAAM,CAAC;gBACN,WAAW,GAAG,yBAAyB,cAAc,yBAAyB,OAAO,EAAE,CAAC;YAC1F,CAAC;QACH,CAAC;QAED,oCAAoC;QACpC,IAAI,KAAK,IAAI,KAAK,KAAK,IAAI,IAAI,KAAK,KAAK,SAAS,IAAI,KAAK,CAAC,IAAI,EAAE,KAAK,EAAE,EAAE,CAAC;YAC1E,WAAW,GAAG,qCAAqC,KAAK,gLAAgL,KAAK,wJAAwJ,WAAW,EAAE,CAAC;QACrZ,CAAC;QAED,OAAO,CAAC,GAAG,CAAC,iCAAiC,mBAAmB,CAAC,CAAC,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,oBAAoB,CAAC,CAAC;QAEvH,MAAM,QAAQ,GAAG,MAAM,UAAU,CAAC,IAAI,CAAC,EAAE,OAAO,EAAE,WAAW,EAAE,CAAC,CAAC;QACjE,OAAO,CAAA,QAAQ,aAAR,QAAQ,uBAAR,QAAQ,CAAE,QAAQ,KAAI,0BAA0B,CAAC;IAC1D,CAAC;IAAC,OAAO,KAAK,EAAE,CAAC;QACf,OAAO,CAAC,KAAK,CAAC,yBAAyB,EAAE,KAAK,CAAC,CAAC;QAEhD,2BAA2B;QAC3B,OAAO,yPAAyP,CAAC;IACnQ,CAAC;AACH,CAAC","sourcesContent":["import { Settings, LlamaCloudIndex, ContextChatEngine } from \"llamaindex\";\r\nimport { Gemini, GEMINI_MODEL } from \"@llamaindex/google\";\r\nimport { readFileSync } from \"fs\";\r\nimport { join } from \"path\";\r\nimport { config } from 'dotenv';\r\n\r\nconfig();\r\n\r\nexport async function llamaChat(\r\n  message: string, \r\n  topic?: string, \r\n  conversationHistory?: Array<{ role: string; content: string }>\r\n): Promise<string> {\r\n  try {\r\n    const apiKey = process.env.GEMINI_API_KEY;\r\n    if (!apiKey) throw new Error(\"Missing Google API key.\");\r\n\r\n    // Read the system prompt from instruction.md\r\n    const systemPrompt = readFileSync(join(process.cwd(), \"docs\", \"instruction.md\"), \"utf-8\");\r\n\r\n    // ‚úÖ Set up the Gemini model \r\n    Settings.llm = new Gemini({\r\n      apiKey,\r\n      model: GEMINI_MODEL.GEMINI_2_5_FLASH_PREVIEW,\r\n      temperature: 0.5,\r\n    });\r\n\r\n    // ‚úÖ Create index and retriever\r\n    const index = new LlamaCloudIndex({\r\n      name: \"Alvin\",\r\n      projectName: \"Default\",\r\n      organizationId: \"3d06cd16-3374-4c22-b277-a87fb8e6bc14\",\r\n      apiKey: process.env.LLAMA_CLOUD_API_KEY!,\r\n    });\r\n\r\n    const retriever = index.asRetriever({ similarityTopK: 5 });\r\n\r\n    // ‚úÖ Initialize the chat engine with system prompt\r\n    const chatEngine = new ContextChatEngine({ \r\n      retriever,\r\n      systemPrompt: systemPrompt\r\n    });\r\n\r\n    // ‚úÖ Build the user message with context\r\n    let userMessage = message;\r\n    \r\n    // Add conversation history context if provided (very conservative)\r\n    if (conversationHistory && conversationHistory.length > 0) {\r\n      console.log(`üìù Processing conversation history with ${conversationHistory.length} messages`);\r\n      \r\n      // Very conservative limits to prevent API crashes\r\n      const MAX_MESSAGES = 6; // Increased to 6 messages to better capture context\r\n      const MAX_CHARS_PER_MESSAGE = 400; // Increased to 400 chars per message\r\n      \r\n      // Take only the most recent messages\r\n      const recentMessages = conversationHistory.slice(-MAX_MESSAGES);\r\n      \r\n      const processedMessages = recentMessages\r\n        .filter(msg => msg && typeof msg.role === 'string' && typeof msg.content === 'string')\r\n        .map((msg, index) => {\r\n          const sanitizedContent = msg.content.length > MAX_CHARS_PER_MESSAGE \r\n            ? msg.content.substring(0, MAX_CHARS_PER_MESSAGE) + '...'     : msg.content;\r\n          return `${msg.role}: ${sanitizedContent}`;\r\n        });\r\n      \r\n      const historyContext = processedMessages.join('\\n');\r\n      \r\n      console.log(`üìä Context summary: ${processedMessages.length} messages`);\r\n      \r\n      // Enhanced context formatting for better follow-up question handling\r\n      const isFollowUpQuestion = message.toLowerCase().includes('yes') || \r\n                                message.toLowerCase().includes('explain') || \r\n                                message.toLowerCase().includes('more') ||\r\n                                message.toLowerCase().includes('detail');\r\n      \r\n      if (isFollowUpQuestion) {\r\n        userMessage = `IMPORTANT: This appears to be a follow-up question. The user is asking for more details or clarification about the previous conversation. Please refer back to the specific topic or question from the conversation history and provide comprehensive, detailed information about it.\\n\\nRecent conversation:\\n${historyContext}\\n\\nCurrent follow-up question: ${message}`;\r\n      } else {\r\n        userMessage = `Recent conversation:\\n${historyContext}\\n\\nCurrent question: ${message}`;\r\n      }\r\n    }\r\n    \r\n    // Add topic constraint if specified\r\n    if (topic && topic !== null && topic !== undefined && topic.trim() !== '') {\r\n      userMessage = `IMPORTANT: The user has selected \"${topic}\" as the current topic. You must ONLY answer questions about this topic. If the user asks about a different topic, politely remind them that the current chat is focused on \"${topic}\" and suggest they clear their selection to ask about something else. Use the exact response format from the system prompt for topic redirection.\\n\\n${userMessage}`;\r\n    }\r\n\r\n    console.log(`ü§ñ Sending message to AI with ${conversationHistory ? conversationHistory.length : 0} previous messages`);\r\n\r\n    const response = await chatEngine.chat({ message: userMessage });\r\n    return response?.response || \"No response from Gemini.\";\r\n  } catch (error) {\r\n    console.error(\"‚ùå LlamaCloud API Error:\", error);\r\n    \r\n    // Simple fallback response\r\n    return \"I'm experiencing technical difficulties. Please try asking your question again, or contact HR directly for immediate assistance. You can also visit our HR System documents at https://office.Proweaver.tools/hrsystem/memo for additional information.\";\r\n  }\r\n} "]}
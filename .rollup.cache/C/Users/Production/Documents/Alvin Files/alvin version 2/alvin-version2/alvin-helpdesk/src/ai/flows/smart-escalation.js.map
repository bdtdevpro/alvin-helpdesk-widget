{"version":3,"file":"smart-escalation.js","sourceRoot":"","sources":["smart-escalation.ts"],"names":[],"mappings":"AAAA,YAAY,CAAC;AAEb;;;;;;GAMG;AAEH,OAAO,EAAE,QAAQ,EAAE,eAAe,EAAE,iBAAiB,EAAE,MAAM,YAAY,CAAC;AAC1E,OAAO,EAAE,MAAM,EAAE,YAAY,EAAE,MAAM,oBAAoB,CAAC;AAC1D,OAAO,EAAE,CAAC,EAAE,MAAM,KAAK,CAAC;AAExB,iCAAiC;AACjC,MAAM,MAAM,GAAG,OAAO,CAAC,GAAG,CAAC,cAAc,CAAC;AAC1C,IAAI,CAAC,MAAM;IAAE,MAAM,IAAI,KAAK,CAAC,yBAAyB,CAAC,CAAC;AACxD,QAAQ,CAAC,GAAG,GAAG,IAAI,MAAM,CAAC;IACxB,MAAM;IACN,KAAK,EAAE,YAAY,CAAC,wBAAwB;IAC5C,WAAW,EAAE,GAAG;CACjB,CAAC,CAAC;AAEH,MAAM,0BAA0B,GAAG,CAAC,CAAC,MAAM,CAAC;IAC1C,SAAS,EAAE,CAAC,CAAC,MAAM,EAAE,CAAC,QAAQ,CAAC,sCAAsC,CAAC;IACtE,mBAAmB,EAAE,CAAC;SACnB,MAAM,EAAE;SACR,QAAQ,EAAE;SACV,QAAQ,CAAC,gDAAgD,CAAC;CAC9D,CAAC,CAAC;AAGH,MAAM,2BAA2B,GAAG,CAAC,CAAC,MAAM,CAAC;IAC3C,UAAU,EAAE,CAAC;SACV,MAAM,EAAE;SACR,QAAQ,CACP,iFAAiF,CAClF;IACH,MAAM,EAAE,CAAC;SACN,MAAM,EAAE;SACR,QAAQ,CAAC,yDAAyD,CAAC;CACvE,CAAC,CAAC;AAGH,yEAAyE;AACzE,MAAM,KAAK,GAAG,IAAI,eAAe,CAAC;IAChC,IAAI,EAAE,OAAO;IACb,WAAW,EAAE,SAAS;IACtB,cAAc,EAAE,sCAAsC;IACtD,MAAM,EAAE,OAAO,CAAC,GAAG,CAAC,mBAAmB;CACxC,CAAC,CAAC;AACH,MAAM,SAAS,GAAG,KAAK,CAAC,WAAW,CAAC,EAAE,cAAc,EAAE,CAAC,EAAE,CAAC,CAAC;AAC3D,MAAM,UAAU,GAAG,IAAI,iBAAiB,CAAC,EAAE,SAAS,EAAE,CAAC,CAAC;AAExD,MAAM,CAAC,KAAK,UAAU,eAAe,CACnC,KAA2B;IAE3B,+BAA+B;IAC/B,MAAM,MAAM,GAAG,sXAAsX,KAAK,CAAC,SAAS,2BAA2B,KAAK,CAAC,mBAAmB,IAAI,QAAQ,uIAAuI,CAAC;IAE5lB,MAAM,QAAQ,GAAG,MAAM,UAAU,CAAC,IAAI,CAAC,EAAE,OAAO,EAAE,MAAM,EAAE,CAAC,CAAC;IAE5D,yDAAyD;IACzD,kDAAkD;IAClD,MAAM,IAAI,GAAG,OAAO,QAAQ,KAAK,QAAQ,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC;IACzE,MAAM,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC,0CAA0C,CAAC,CAAC;IACrE,IAAI,KAAK,EAAE,CAAC;QACV,OAAO;YACL,UAAU,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE;YAC3B,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE;SACxB,CAAC;IACJ,CAAC;SAAM,CAAC;QACN,oEAAoE;QACpE,OAAO;YACL,UAAU,EAAE,SAAS;YACrB,MAAM,EAAE,IAAI,CAAC,IAAI,EAAE;SACpB,CAAC;IACJ,CAAC;AACH,CAAC","sourcesContent":["'use server';\r\n\r\n/**\r\n * @fileOverview AI-powered smart escalation flow to route complex inquiries to the appropriate department.\r\n *\r\n * - smartEscalation - A function that handles the escalation process.\r\n * - SmartEscalationInput - The input type for the smartEscalation function.\r\n * - SmartEscalationOutput - The return type for the smartEscalation function.\r\n */\r\n\r\nimport { Settings, LlamaCloudIndex, ContextChatEngine } from \"llamaindex\";\r\nimport { Gemini, GEMINI_MODEL } from \"@llamaindex/google\";\r\nimport { z } from \"zod\";\r\n\r\n// Set up the LLM at module level\r\nconst apiKey = process.env.GEMINI_API_KEY;\r\nif (!apiKey) throw new Error(\"Missing Google API key.\");\r\nSettings.llm = new Gemini({\r\n  apiKey,\r\n  model: GEMINI_MODEL.GEMINI_2_5_FLASH_PREVIEW,\r\n  temperature: 0.1,\r\n});\r\n\r\nconst SmartEscalationInputSchema = z.object({\r\n  userQuery: z.string().describe('The user query or issue description.'),\r\n  conversationHistory: z\r\n    .string()\r\n    .optional()\r\n    .describe('The history of the conversation, if available.'),\r\n});\r\nexport type SmartEscalationInput = z.infer<typeof SmartEscalationInputSchema>;\r\n\r\nconst SmartEscalationOutputSchema = z.object({\r\n  department: z\r\n    .string()\r\n    .describe(\r\n      'The appropriate department to escalate the inquiry to (e.g., HR, IT, Benefits).'\r\n    ),\r\n  reason: z\r\n    .string()\r\n    .describe('The reason for escalating to the identified department.'),\r\n});\r\nexport type SmartEscalationOutput = z.infer<typeof SmartEscalationOutputSchema>;\r\n\r\n// Initialize LlamaIndex (do this once, outside the function if possible)\r\nconst index = new LlamaCloudIndex({\r\n  name: \"Alvin\",\r\n  projectName: \"Default\",\r\n  organizationId: \"f0ffa7f0-4c56-4818-88e6-65cd26049bae\",\r\n  apiKey: process.env.LLAMA_CLOUD_API_KEY,\r\n});\r\nconst retriever = index.asRetriever({ similarityTopK: 5 });\r\nconst chatEngine = new ContextChatEngine({ retriever });\r\n\r\nexport async function smartEscalation(\r\n  input: SmartEscalationInput\r\n): Promise<SmartEscalationOutput> {\r\n  // Compose the prompt as before\r\n  const prompt = `You are an AI assistant designed to analyze user inquiries and conversation history to determine the most appropriate department for escalation.\\n\\nBased on the user's query and any available conversation history, identify the relevant department (e.g., HR, IT, Benefits, Payroll) to handle the issue. Provide a brief reason for your department selection.\\n\\nUser Query: ${input.userQuery}\\nConversation History: ${input.conversationHistory || \"(none)\"}\\n\\nRespond with the department name and a brief explanation of why that department is most suitable for addressing the user's needs.`;\r\n\r\n  const response = await chatEngine.chat({ message: prompt });\r\n\r\n  // Try to extract department and reason from the response\r\n  // (Assume response.response is the string answer)\r\n  const text = typeof response === 'string' ? response : response.response;\r\n  const match = text.match(/Department:\\s*(.+)\\nReason:\\s*([\\s\\S]+)/i);\r\n  if (match) {\r\n    return {\r\n      department: match[1].trim(),\r\n      reason: match[2].trim(),\r\n    };\r\n  } else {\r\n    // Fallback: return the whole response as reason, department unknown\r\n    return {\r\n      department: \"Unknown\",\r\n      reason: text.trim(),\r\n    };\r\n  }\r\n}\r\n"]}